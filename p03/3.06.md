## 3.6. Dado el algoritmo de construcción de árboles visto en clase para atributos continuos: Sea $S$ una muestra de instancias con atributos $A$. Para construir un árbol de decisión ejecutamos:

1. Elegimos el par $a \in A, c \in \mathbb{R}$ entre los posibles pares (atributo, corte), que mejor divida a $S$ para `nodo_actual`
según $\Delta M(S, (a, c))$ (define cuánto gano si divido a $S$ en regiones según la medida $M$).
    
2. Crear dos hijos del `nodo_actual`.

3. Dividir las instancias de $S$ en los nuevos nodos, según $(a, c)$: 

    $S_\leq\leftarrow\{x\ |\ x \in S\ \wedge\ x[a] \leq c\}$

4. Repetir para cada hijo hasta que se cumpla el criterio de corte

### a) Escribir el pseudocódigo (puede ser similar a python) para el paso 1: elegir el mejor par $(a, c)$ entre los posibles pares. Es decir, definir mejor_corte($S, A, \Delta M$), en donde $S$ representa la muestra, $A$ un conjunto de atributos y $|\Delta M|$ la función que computa la ganancia de una división. Puede suponer dadas funciones tales como $S[a]$ que devuelve la columna de valores para el atributo $a$. Es importante que esta función no testee dos veces cortes que devuelven exactamente las mismas regiones para un mismo atributo.

Ver `best_cut_cont` en [dta.py](./dta.py).

### b) Introducir los cambios necesarios en el algoritmo general y en la función del punto (a) para poder medir la importancia de cada atributo.

Ver `CART` en [dta.py](./dta.py). El árbol construido guarda la ganancia obtenida en cada decisión. Un algoritmo puede recorrer el árbol para medir la importancia de cada atributo (medida como el decrecimiento total en la impureza).
