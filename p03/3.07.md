## 3.7. Determinar cuáles de las siguientes son afirmaciones verdaderas:

### a) El objetivo de construir un árbol de decisión es crear el árbol de menor tamaño posible en el cual las hojas contengan valores de una sóla clase.

Falso. El objetivo es crear una aproximación de una función objetivo que capture lo mejor posible la relación entre un modelo (definido por medio de una serie de atributos para nosotros relevantes) y un concepto. 

### b) Los algoritmos de construcción vistos (CART, ID3, etc) exploran todos los posibles árboles y se quedan con el que mejor separa a las instancias

Falso. CART e ID3 reducen el espacio de busqueda por medio de heurísticas greedy. Se quedan con un árbol que separa a las instancias de la 'mejor' manera en base a distintos sesgos. Entre ellos, el sesgo de la métrica utilizada para comparar atributos y su preferencia por árboles de tamaño chico, siguiendo la filosofía de la navaja de occam.

### c) La pureza describe qué tan cerca está un nodo de contener instancias de una sola clase.

Verdadero.

### d) Un atributo puede aparecer sólo una vez en cada rama del árbol (llamamos rama a un camino directo desde una hoja hasta la raíz).

Verdadero para el caso categórico (usando ID3) y Falso para el caso continuo (usando CART).

### e) Un par (atributo, corte) puede aparecer sólo una vez en cada rama del árbol (llamamos rama a un camino directo desde una hoja hasta la raíz).

Verdadero. Realizar el mismo corte no puede aporta ninguna mejora, ya que no va a lograr particionar los datos nuevamente. 

### f) Para cada nueva instancia, un árbol permite predecir la clase a la que pertence. Por otra parte, para predecir la probabilidad de pertencer a una clase u otra, es necesario modificar el algoritmo de creación de árboles.

Verdadero. 

### g) Un árbol de decisión, con criterios de corte suficientemente laxos, puede siempre conseguir 100 % de aciertos en los datos de entrenamiento.

Falso, ver (h).

### h) Un árbol de decisión, con criterios de corte suficientemente laxos, puede siempre conseguir 100 % de aciertos en los datos de entrenamiento siempre y cuando no haya contradicciones entre las etiquetas de instancias iguales.

Si hay contradicciones, está claro que no se va a poder conseguir 100 % de aciertos. Entendiendo 'criterio de corte' como 'criterio de parada', un árbol con un criterio de corte lo suficientemente laxo puede tener una hoja por instancia. Luego, puede 'memorizar' todos los datos de entrenamiento y lograr un 100% de aciertos.
