## 3.9. Preguntas conceptuales para discutir:

### a) ¿Cuál el sesgo inductivo del algoritmo que construye el árbol de decisión?

El sesgo inductivo del algoritmo es:  
- decidir aproximarse a la función objetivo por medio de la partición del espacio de búsqueda en subconjuntos separados.
- el criterio que utiliza para definir el mejor corte es heurístico.
- ID3 tiene una preferencia por árboles chicos y que ponen aquellos nodos que presentan mayor `info_gain` más cerca de la raíz.

### b) ¿Qué sucede cuando dos atributos empatan en ganancia de información? ¿Esta decisión es parte del sesgo inductivo?

De acuerdo al algoritmo, se elige uno u otro. Tal como se definió en [dta.py](./dta.py), se elige el primer atributo analizado cuya ganancia sea máxima. Esta decisión es parte del sesgo inductivo, ya que es parte implícita del criterio de decisión.    

### c) ¿Cómo se comporta la ganancia de información en comparación a impureza Gini cuando se comparan atributos con gran cantidad de valores distintos? Por ejemplo, si el atributo $x_1$ tiene dos valores posibles (true y false) y el atributo $x_2$ tiene 40 valores distintos, ¿es justo usar ganancia de información para elegir entre ellos? ¿Qué desventajas tiene? ¿Cómo se podría mitigar?

Esta pregunta es un poco extraña, dado que la impureza Gini no cumple el mismo rol que la ganancia de información en los arboles de decisión. Tendría más sentido comparar la ganancia Gini con la ganancia de información, o la impureza Gini con Entropía. 

Otra cosa que complica la comparación es que el atributo particular importa en las fórmulas de ganancia, ya que determina (junto a un corte en el caso continuo) cómo se propone particionar el conjunto $S$ de instancias bajo observación. Pero no importa en las medidas de impureza, ya que estas últimas realizan la medición en base a la clasificación de las instancias (los valores que el label puede tomar) y no los valores posibles de un atributo particular.

Así también, en las métricas de ganancia, importa si el criterio de partición es por corte (lo que implica la capacidad de ordenar los valores del atributo) o por clases (donde cada valor del atributo define la propia).

Dicho esto, creo que tiene más sentido comparar ganancia Gini con ganancia de información en el contexto de un atributo categórico que se particionará por clases definidas por los valores posibles del atributo.

En este sentido, ganancia de información y ganancia Gini tienen el siguiente problema: favorecen a los atributos con mayor cantidad de valores. Esto se debe a que la partición del atributo se va a componer de muchos subconjuntos chicos, y la impureza de cada uno será más chica. Ambas medidas son bastante similares. De hecho, dado que la impureza de Gini toma valores por debajo de Entropía, es esperable que sea más propensa a este comportamiento.

Una forma de solucionar esto es penalizar la partición en muchas clases. Por ejemplo, usando la tasa de ganancia de información (information gain ratio).

### d) Dado un atributo a continuo en la cual queremos encontrar el mejor corte $c$ según alguno de los criterios considerados (Gini o Entropy), ¿cuál es la complejidad de peor caso si se asume que tenemos $n$ instancias en nuestro conjunto de datos?

Los posibles cortes se eligen a partir de los distintos valores que puede tomar el atributo en las instancias. Luego, en el peor caso, se evalúan $n$ cortes. La complejidad de peor caso es entonces $O(n \cdot \omega)$ donde $\omega$ es la complejidad de evaluar el criterio para ese corte. Como mínimo, $\omega \geq O(n)$, ya que implica crear una partición de un conjunto de $n$ instancias.
