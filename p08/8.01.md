## 8.1. Dar una explicación general del algoritmo Bagging.

Bagging es un método, más que un algoritmo, que busca aprovechar la siguiente observación: podemos reducir la varianza de un proceso de aprendizaje si, en vez de construir un solo modelo, construimos una conjunto de modelos (pensarlo como observaciones de una misma variable aleatoria) y promediamos sus predicciones. Esta idea se sustenta en que cada modelo se supone independiente del resto. 

Luego, si el modelo original tiene varianza $\sigma^2$, podemos esperar que el modelo resultante de bagging tenga varianza $\sigma^2/n$, con $n$ la cantidad de modelos entrenados.

Notar que para que esta suposición sea valida, cada modelo debe ser entrenado a partir de un set de datos diferente. Dada la dificultad de contar con una cantidad grande de datos, se suele aplicar el método de bootstrap para formar datasets relativamente distintos (de manera resumida, el método de bootstrap toma una muestra de un dataset de manera aleatoria y con reposición).

Notar que todo modelo se considera igualmente importante para la predicción final y que ninguno de ellos se especializa de manera alguna. El objetivo de esta técnica es reducir la varianza, reduciendo (idealmente) esta parte del error en modelos más complejos.
