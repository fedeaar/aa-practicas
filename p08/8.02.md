## 8.2. Dar una explicación general del algoritmo Random Forest. Basar la explicación en el algoritmo original presentado en el artículo: Breiman, Leo. "Random Forests." Machine learning 45.1 (2001). Incluir:

### a) ¿Cuál es su principal diferencia con Bagging?

### b) ¿Cómo funciona la estimación de error “out-of-bag”? 

### c) ¿Cómo propone Breiman medir la importancia de features? ¿Qué diferencia hay con la manera en que la importancia se mide en el paquete sckit-learn de python?

Breiman presenta un algoritmo con mayor sustento teórico. Su algoritmo busca minimizar la correlación entre los árboles y, al mismo tiempo, mantener la fuerza individual de cada uno, donde la fuerza se define como la esperanza de una función de márgen que, intuitivamente, mide cual es la diferencia entre el voto que obtiene una clasificación correcta de una instancia contra el mejor voto que se obtiene al clasificar la instancia como perteneciente a otra clase. Se podría considerar como una medida de confianza, donde un valor negativo indica que la instancia se clasificaría erroneamente. Em función de estas dos variables se puede acotar el error de generalización del modelo.

A diferencia de bagging aplicado a árboles, este algoritmo -en su versión más simple- entrena los arboles usando samples aleatorias de los atributos al momento de evaluar el mejor corte. Incluso tomando un solo feature, se obtienen buenos resultados. Sin embargo, utiliza también bootstrap para formar los sets de entrenamiento. La razón para esta segunda decisión es la siguiente: permite estimar el error de generalización durante el entrenamiento, al igual que de la correlación y fuerza. Otra diferencia es, en una versión más madura del algoritmo, usar combinaciones lineales de los atributos para aumentar la dimensión de la instancias.

La estimación del error se hace 'out-of-bag': para cada instancia $x$, solo los modelos que no fueron entrenados sobre ella votan respecto a su clasificación. Luego, la predicción obtenida, evaluada sobre alguna métrica, es una estimación del error de generalización bajo las mismas suposiciones que si se hubiera usado un set de evaluación. En el paper, cita evidencia empírica respecto a que usar esta forma de estimación es tan exacta con tener un set de evaluación del mismo tamaño que el de entrenamiento. Como critica, menciona que se sobrestima el error, hasta converger. Pero, al menos, no se incurre en sesgo, a diferencia de usar validación cruzada.

Otro aspecto interesante es que un propone método para la interpretabilidad del modelo resultante: medir la importancia de cada atributo por el error 'out of bag' que induce 'remover', por turno, la importancia de cada atributo permutando aleatoriamente sus valores en las instancias. Esto es similar a como se mide la importancia de atributo en árboles.
