## 8.6. Explicar la diferencia entre AdaBoost y GradientBoosting.

GradientBoosting es una técnica de boosting, que, al igual que AdaBoost, realiza un refinamiento aditivo para aprender, lentamente, un modelo. En este caso, considera como residuo al gradiente ponderado respecto a una función de pérdida arbitraria (AdaBoost, en cambio, deriva la forma del residuo en base a la función de pérdida exponencial). Si bien se debería poder extender para usar con otros modelos base, GradientBoosting se suele aplicar con árboles y difiere de AdaBoost también en lo siguiente: entrenado el $m$-ésimo árbol en función del residuo, se buscan los parámetros $\gamma_i$ que minimicen, por región terminal $r_i$, $1 \leq i \leq J$, la pérdida respecto a $f_{m-1} + \gamma_i$ y se define $$f_m(x) = f_{m-1}(x) + \sum_{j=1}^{J}\gamma_j I(x \in r_j)$$
Se podría considerar que cada $\gamma_i$ es un stump, tal que en cada paso, no se agrega un solo modelo, sino $J$ modelos muy sencillos. 
