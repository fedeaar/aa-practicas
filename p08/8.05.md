## 8.5. Explicar la idea conceptual del meta-algoritmo AdaBoost. Incluir:

### a) ¿Qué significa “weak-learners”?

### b) ¿Cómo se calculan los pesos para las instancias en cada iteración?

### c) ¿Cuál es el criterio para determinar la cantidad de modelos en el ensamble?

AdaBoost es un meta-algoritmo que utiliza el método de Boosting. En este, se entrena una serie de modelos de manera secuencial, utilizando en cada uno información de los anteriores. Esta información se codifica como pesos en el dataset de entrenamiento. 

La idea del método es la siguiente: entrenar de manera lenta (el algoritmo es un weak-learner). Esto es, dado el ultimo modelo entrenado dentro del conjunto, se utiliza el residuo, ajustado por algún parámetro de contracción, en vez de las clases, como la respuesta para entrenar al próximo modelo (esto puede implicar la modificación del algoritmo de aprendizaje utilizado). Luego, el proximo modelo aprenderá donde los anteriores no. La decisión final se pondera entre todos los sub-modelos que componen al modelo de acuerdo a alguna medida de la importancia de cada uno y se basa en el signo resultante.

La cantidad de modelos en el ensamble, por su parte, se decide por medio de validación cruzada. Por ejemplo, usando una curva de complejidad para evitar sobreajuste.

Volviendo a AdaBoost, ESL considera que el algoritmo es una forma de modelado aditivo de a etapas: secuencialmente, se agregan funciones bases a una expansión aditiva sin ajustar los parámetros y coeficientes que ya la componen. En cada paso, se entrena un modelo y se busca el coeficiente óptimo con el que ponderarlo para luego agregarlo a la expansión actual. En este caso, se utiliza la función de pérdida $e^(-y\cdot h(x))$ para derivar el coeficiente óptimo, que resulta ser $$\beta_m = \frac{1}{2}\log\frac{1 - err_m0}{err_m}$$ 
con $err_m$ el error de clasificación del $m$-ésimo modelo, ponderado por los coeficientes derivados de los pasos anteriores para cada instancia.

Los pesos del dataset, por su parte, se actualizan de la siguiente forma
$$
    D_{m+1}[i] = \frac{D_m[i]}{Z_m}e^{-\beta_m y^{(i)} h_t(x^{(i)})}
$$
con $Z_t$ una constante de normalización que logra que $sum(D_{m+1}) = 1$.