## 5.7. Verdadero o Falso (justificar)

### a) Tanto recall como precision no toman en cuenta qué tan bien el modelo maneja los casos negativos.

Verdadero. PR y REC son métricas sobre los errores de clasificación en la clase positiva. Notar que, podemos medir la performance sobre la clase negativa cambiando la interpretación de las etiquetas.

### b) Un modelo que no produce falsos positivos tiene $precision = 1.0$.

A medias. Por definición si FP = 0, entonces PR = TP / TP = 1. Salvo que TP = 0, en tal caso se indefine.  

### c) Un modelo que no produce falsos negativos tiene $recall = 1.0$.

A medias. Por definición si FN = 0, entonces REC = TP / TP = 1. Salvo que TP = 0, en tal caso se indefine.  

### d) Si un clasificador devuelve probabilidades, la matriz de confusión se construye de manera ponderada según la probabilidad de cada clase.

Falso (al menos según lo que busqué). Si bien es una idea interesante, no sería una matriz de confusión 'clásica'. La forma común sería fijar un umbral de clasificación y construir la matríz de confusión en base a la decisión tomada con ese umbral. 

### e) Si un clasificador devuelve probabilidades, hay muchas matrices de confusión asociadas dependiendo del umbral de clasificación.

Verdadero, en tanto haya más de una instancia a clasificar. Ver (d) y (f).

### f) Si un clasificador devuelve probabilidades, hay infinitas matrices de confusión asociadas dependiendo del umbral de clasificación.

Verdadero si se permite variar el conjunto de evaluación. Falso si es siempre el mismo. Esto se debe a que los umbrales que producirán un cambio en la matriz de confusión son a lo sumo tantos como la cantidad de instancias.

### g) Aumentar el umbral de clasificación produce que la precision siempre suba.

Falso. Ver [5.02.e](./5.02.md).

### h) Aumentar el umbral de clasificación produce que el recall baje o se mantenga igual.

Verdadero. Ver [5.02.f](5.02.md).

### i) La métrica precision es parte fundamental del cálculo de la curva ROC.

Falso. ROC usa TPR (recall) y FPR.
