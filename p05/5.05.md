## 5.5. Verdadero o Falso (justificar)

### a) El AUC-ROC es invariante de escala. Mide qué tan bien se clasifican las predicciones, en lugar de sus valores absolutos.

Verdadero. El AUC-ROC es una medida del area bajo la curva TPR (recall) - FPR y es invariante de la escala del output del modelo (si importa al momento de tomar el rango de umbrales sobre el cual medir estas métricas). Esto es bastante útil a la hora de comparar modelos.    

### b) En caso que el ordenamiento de las instancias sea el mismo, AUC-ROC va poder distinguir un clasificador muy confiado (instancias clasificadas como positiva tienen scores muy altos, instancias clasificadas como negativas scores muy bajos) de uno poco confiado (scores menos extremos).

Falso. A AUC-ROC solo le interesa el area bajo la curva ROC. La curva ROC para un clasificador muy 'confiado' va a empezar (como siempre) con TPR y FPR = 1 para u = 0, va a decrecer hasta llegar al umbral máximo de clasificación negativa, se va a mantener estable hasta llegar al umbral mínimo de clasificación positiva y probablemente proceda a caer estrepitósamente (obs. que vamos de derecha a izquierda en la curva). Uno 'poco' confiado, va a tener una curva mas gradual. En cualquiera de ambos casos, el area bajo la curva puede resultar igual.

Sí debería ser posible de distinguir estos aspectos de los modelos contrastando sus curvas ROC.

### c) AUC-ROC es invariante de umbral de clasificación. Mide la calidad de las predicciones del modelo independientemente del umbral de clasificación elegido.

Verdadero. AUC-ROC considera -todos- los umbrales de clasificación en simultáneo.

### d) En los casos en los que existen grandes disparidades en el costo de los falsos negativos frente a los falsos positivos, puede ser fundamental minimizar un tipo de error de clasificación. Por ejemplo, al detectar spam, es probable que desee priorizar la minimización de los falsos positivos (incluso si eso resulta en un aumento significativo de los falsos negativos). AUC-ROC no es la mejor métrica para este tipo de optimización.

Verdadero. AUC-ROC mide la capacidad del modelo de asignarle mayor confianza a instancias de la clase positiva y menor confianza a instancias de la clase negativa. Se podría considerar, con igual peso para ambos tipos de errores. Como podría interesar reducir la performance en un tipo de error que en el otro, no siempre es la mejor métrica.
