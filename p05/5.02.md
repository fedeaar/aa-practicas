## 5.2. Métricas de umbral fijo

### a) Explicar con tus palabras la definición de `accuracy`, `precision` y `recall`.

`accuracy`: la proporción de aciertos sobre el total de instancias evaluadas.

`precision`: la proporción de verdaderos positivos sobre el total de instancias clasificadas como positivas. Se puede pensar que es inversamente proporcional a la cantidad de falsos positivos.

`recall`: la proporción de verdaderos positivos sobre el total entre falsos negativos y verdaderos positivos. Se puede pensar que es inversamente proporcional a la cantidad de falsos negativos.

### b) Completar la Segunda Parte del [notebook_metricas.ipynb](./notebook_06_metricas-published.ipynb). El Test 2 debería pasar.

Ver [notebook_metricas.ipynb](./notebook_06_metricas-published.ipynb).

### c) ¿Por qué es un problema medir accuracy de un clasificador para compararlo con otro? Dar un ejemplo en donde sería engañoso utilizar esta comparación.

Al resumir performance por medio de accuracy, se está perdiendo la información relacionada a los falsos positivos y falsos negativos. En problemas donde el peso de cada error es distinto, esto es problematico. Si se compara por accuracy, se pierde esta información. 

Por ejemplo, si un problema tiene un desbalance importante entre clases, un modelo 'bobo' que clasifique siempre la clase mayoritaria muy posiblemente tenga mejor accuracy que otro que sepa clasificar mejor la clase minoritaria a cuestas de una peor performance en la clase mayoritaria.

### d) Demuestre que $F_\beta$ puede ser reescrito como $$F_\beta = \frac{(1 + \beta^2) ·TP}{(1 + \beta^2) \cdot TP + \beta^2 \cdot FN + FP}$$

Por definición,
$$
\begin{split}
    F_\beta 
        &:= (1 + \beta^2)\frac{PR\cdot RC}{\beta^2 \cdot PR + RC}\\
        &= (1 + \beta^2)\frac{\frac{TP}{TP + FP}\cdot \frac{TP}{TP + FN}}{\beta^2 \cdot \frac{TP}{TP + FP} + \frac{TP}{TP + FN}}\\
        &= (1 + \beta^2)\frac{\frac{TP^2}{(TP + FP)\cdot(TP + FN)}}{\frac{\beta^2 \cdot TP\cdot(TP + FN) + TP\cdot(TP + FP)}{(TP + FP)\cdot(TP + FN)}}\\
        &= (1 + \beta^2)\frac{TP}{\beta^2 \cdot (TP + FN) + TP + FP}\\
        &= \frac{(1 + \beta^2) \cdot TP}{(1 + \beta^2) \cdot TP + \beta^2 \cdot FN + FP}
         
\end{split}
$$

$\blacksquare$

### e) Demuestre que la métrica recall vista como función del umbral de decisión, es una función monótona decreciente ($recall_{M,D}(\mu_1) \leq recall_{M,D}(\mu_2)$ si $\mu_1 > \mu_2$)

Voy a definir $recall_{M,D}(\mu)$  como el valor de recall dado que el modelo $M$ sobre los datos $D$ decide que una instancia $x$ es positiva si $proba_{M}(x) > \mu$.

Supongamos que la función no es monótona decreciente. Luego, existe $\mu_1, \mu_2 \in [0, 1]$, $\mu_1 > \mu_2$, tal que $recall_{M,D}(\mu_1) > recall_{M,D}(\mu_2)$.

Como $proba_{M}(x) > \mu_1$ implica por transitividad que $proba_{M}(x) > \mu_2$, entonces la cantidad de verdaderos positivos de $recall_{M,D}(\mu_1)$ es menor o igual a la cantidad de verdaderos positivos de $recall_{M,D}(\mu_2)$, ya que las instancias que se clasifican positivas con $\mu_1$ son un subconjunto de las instancias que se clasifican del mismo modo con $\mu_2$. Esto es $TP_{\mu_1} \leq TP_{\mu_2}$. 

Por otro lado, notar que, por definición, $TP_{\mu_1} + FN_{\mu_1} = TP_{\mu_2} + FN_{\mu_2}$. Esto se debe a que $TP + FN = P$, donde $P$ es la cantidad de instancias positivas 'en la realidad' (en el dataset).

Sigue que
$$
\begin{split}
    recall_{M,D}(\mu_1) = \frac{TP_1}{TP_1 + FN_1} \leq \frac{TP_2}{TP_2 + FN_2} = recall_{M,D}(\mu_2)
\end{split}
$$

$\rightarrow\leftarrow$

$\blacksquare$

### f) Muestre cómo la métrica precision vista como función del umbral de decisión, no necesariamente es una función monótona creciente ($precision_{M,D}(\mu_1) \geq precision_{M,D}(\mu_2)$ si $\mu_1 > \mu_2$).

Podemos considerar el siguiente ejemplo:

Sea $T = \{(x_1, 1),\ (x_2, 0),\ (x_3, 1)\}$ tres instancias para las cuales el modelo $M$ entrenado en $D$ asigna una confianza de 

$$
\begin{split}
    proba_{M, D}(x_1) &= 0.2\\
    proba_{M, D}(x_2) &= 0.4\\
    proba_{M, D}(x_3) &= 0.4
\end{split}
$$
Para la clase $1$. Luego, para el umbral $\mu_1 = 0.1$ tenemos $TP = 2$, $FP = 1$ y
$$
    precision_{M,D}(\mu_1) = \frac{2}{3}
$$
y para $\mu_2 = 0.3$ tenemos $TP = 1$, $FP = 1$ y
$$
    precision_{M,D}(\mu_2) = \frac{1}{2}
$$
Luego, $precision$ no es necesariamente monótona creciente.

$\blacksquare$
