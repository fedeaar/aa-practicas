## 6.6. Sea $\delta_c(x)$ la funciones discriminantes para la clase $c$: $$\delta_c(x) = x \cdot \frac{\mu_c}{\sigma^2} − \frac{\mu^2_c}{2\sigma^2} + \log \pi_c$$ en donde $\pi_c = \hat{P}(Y = c)$

### Bajo las suposiciones de LDA, se puede derivar $$Pred(x^{(i)}) = \argmax_{c\in Clases} P(Y =c\ |\ X=x^{(i)}) = \argmax_{c\in Clases} \delta_c(x^{(i)})$$

### Es decir, para predecir la clase de una instancia, basta con evaluar esta función discriminante para cada clase y conservar la que retorne el mayor valor.

### a) Demostrar esta igualdad.

Tenemos, bajo el sesgo inductivo de LDA con $p=1$ (la definición del enunciado es para este caso),

$$
\begin{split}
    P(Y =c\ |\ X=x^{(i)})
        &= \frac{P(Y = c) P(X = x^{(i)}\ |\ Y = c)}{P(X = x^{(i)})}\\
        &= \frac{\pi_c \cdot pdf_c(x^{(i)})}{\sum_{k=1}^C \pi_k \cdot pdf_k(x^{(i)})}\\
        &= \frac{\pi_c \cdot f_{norm}(x^{(i)}; \mu_c, \sigma)}{\sum_{k=1}^C \pi_k \cdot f_{norm}(x^{(i)}; \mu_k, \sigma)}\\ 
\end{split}
$$
como queremos maximizar este valor, podemos descartar el divisor y tomar el logaritmo natural. Luego,
$$
\begin{split}
    \ln(\pi_c \cdot f_{norm}(x^{(i)}; \mu_c, \sigma)) 
        &= \ln(\pi_c) + \ln(\frac{1}{\sqrt {2\pi} \sigma}e^{-\frac{1}{2\sigma^2}(x^{(i)} - \mu_c)^2})\\
        &=  \ln(\pi_c) + \ln(\frac{1}{\sqrt {2\pi} \sigma}) -\frac{1}{2\sigma^2}(x^{(i)} - \mu_c)^2\\
        &= \ln(\pi_c) + \ln(\frac{1}{\sqrt {2\pi} \sigma}) - \frac{x^{(i)^2}}{2\sigma^2} + x^{(i)}\cdot\frac{\mu_c}{\sigma^2} - \frac{\mu_2^2}{2\sigma^2}
\end{split}
$$
y, como $\ln(\frac{1}{\sqrt {2\pi} \sigma}) - \frac{x^{(i)^2}}{2\sigma^2}$ es constante para todo $c$, llegamos a que

$$
\argmax_{c\in Clases} P(Y =c\ |\ X=x^{(i)}) = \argmax_{c\in Clases} (x^{(i)}\cdot\frac{\mu_c}{\sigma^2} - \frac{\mu_2^2}{2\sigma^2} + \ln(\pi_c)) =  \argmax_{c\in Clases} \delta_c(x^{(i)})
$$

$\blacksquare$

### b) Demostrar que la frontera de decisión para el caso $p = 1, k = 2, \pi_1 = \pi_2$ es $x = (\hat\mu_1 + \hat\mu_2)/2$.

Para encontrar la frontera de decisión, tenemos que ver
$$
\begin{split}
x\cdot\frac{\mu_1}{\sigma^2} - \frac{\mu_1^2}{2\sigma^2} + \ln(\pi_1) = x\cdot\frac{\mu_2}{\sigma^2} - \frac{\mu_2^2}{2\sigma^2} + \ln(\pi_2)
    &\iff x\frac{\mu_1 - \mu_2}{\sigma^2} = \frac{\mu_1^2 - \mu_2^2}{2 \sigma^2}\\
    &\iff x = \frac{\mu_1^2 - \mu_2^2}{2 (\mu_1 - \mu_2)}\\
    &\iff x = \frac{\mu_1 + \mu_2}{2} 
\end{split}
$$

$\blacksquare$

### c) La palabra linear en LDA se debe a que $\delta_k(x)$ es una función lineal en $x$. Mostrar que si se elimina la suposición de que todas las clases tienen la misma varianza, entonces la función discriminante pasa a ser cuadrática en $x$. (A esa técnica se la conoce como quadradic discriminant analysis, o QDA).

Siguiendo con el caso $p = 1$, si no asumimos la misma varianza para toda clase, vemos que  $\ln(\frac{1}{\sqrt {2\pi} \sigma}) - \frac{x^{(i)^2}}{2\sigma^2} = - (\ln(\sqrt{2\pi})  + \ln(\sigma) + \frac{x^{(i)^2}}{2\sigma^2}) $ deja de ser constante.

De manera similar a antes, podemos llegar a que

$$
\begin{split}
\argmax_{c\in Clases} P(Y =c\ |\ X=x^{(i)}) 
    &= \argmax_{c\in Clases} (\frac{x^{(i)^2}}{2\sigma_c^2} + x^{(i)}\cdot\frac{\mu_c}{\sigma_c^2} - \frac{\mu_2^2}{2\sigma_c^2} + \ln(\pi_c\cdot\sigma_c))
\end{split}
$$

que es una función cuadrática en $x$.

$\blacksquare$
