## 4.4. Explicar las posibles causas por las cuales un modelo entrenado y evaluado de la siguientes maneras puede funcionar peor que lo esperado al ser llevado a producción.

### a) Entrenado y evaluado sobre datos de entrenamiento.

La evaluación del modelo probablemente sobrestime su capacidad. Notar que si evaluamos sobre los datos de entrenamiento, siempre vamos a poder 'hackear' el modelo para que sea óptimo en términos de la métrica de evaluación (basta con que se memorize todas las instancias disponibles).

### b) Seleccionado entre muchas posibilidades sobre datos de desarrollo (utilizando grid o random search junto a cross-validation).

Podría darse por varias razones, por ejemplo:

- mala elección de rangos para la selección de hiperparámentros.
- mala elección de la métrica utilizada (no corresponde con los objetivos de producción).
- mala elección del método de validación cruzada utilizada (por ejemplo, no usar cross-validation estratificado (o similar) sobre un dataset con clases no distribuidas equiprobablemente).
- datos de entrenamiento, por mala suerte, sesgados respecto a la distribución real de los mismos.

En particular, asumiendo que la evaluación de referencia para comparar contra producción es la evaluación por cross-validation del modelo con los mejores parámetros sobre entrenamiento, estamos tomando una medida 'optimista' de su comportamiento, ya que los parametros usados están ajustados para optimizar el comportamiento del modelo sobre los datos de entrenamiento.

### c) Seleccionado entre muchas posibilidades sobre datos de desarrollo (utilizando grid o random search junto a cross-validation) y evaluado en datos held-out.

Las mismas razones dadas en los items del punto anterior aplican también en este caso. De todas formas, a diferencia del punto anterior, la medida final que tenemos como referencia es más sobria.
 