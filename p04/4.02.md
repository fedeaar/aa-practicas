## 4.2. Validación cruzada. Verdadero o Falso. Justificar

### a) Hacer validación cruzada evita el sobreajuste (overfitting) de los modelos sobre los datos.

Falso. No necesariamente se va a evitar el sobreajuste. Validación cruzada solo da un marco teórico más sólido (ie. de mayor confianza) sobre el cual evaluar la capacidad de 'generalización' de un modelo. Si el modelo está sobreajustado, uno esperaría que la métrica utilizada para la evaluación lo refleje. Sin embargo, esto está sujeto a diversos factores, como la capacidad descriptiva de la métrica utilizada, el método de distribución en folds utilizado, o si terminamos por incorporar, o no, al set de validación dentro del proceso de desarrollo.

### b) Hacer validación cruzada ayuda a obtener estimaciones más realistas de la performance de un modelo sobre nuevos datos que al hacerlo sobre los mismos datos de entrenamiento.

Verdadero.

### c) En $K$-fold cross validation, conviene que $K$ se acerque a $N$. De esta manera el resultado será lo más realista posible ya que se tiende a generar $N$ modelos independientes. El problema es que hay que entrenar demasiados modelos.

Falso. No necesariamente queremos que $k$ sea cercano a $n$. Esto depende del problema a resolver. Las ventajas de $k$ cercano a $n$ son mayores cuando la cantidad de datos es chica o si queremos tener una noción de qué tan sensible es el proceso de generación de modelos a instancias particulares, por ejemplo. 

### d) Evaluar un modelo sobre el conjunto de evaluación (control) resultará en un valor siempre peor o igual al conseguido en desarrollo.

Falso. No hay razón por la cual una división particularmente favorable no podría resultar en una predicción perfecta, o mejor, que la conseguida durante el desarrollo.

### e) Una vez elegido el mejor modelo durante desarrollo, es conveniente hacer $k$-fold cross validation nuevamente, pero ahora incluyendo también el conjunto de evaluación.

Falso. Se puede, pero el resultado de todo el proceso ya no es un modelo particular sobre el cual tenemos cierta confianza (en términos de alguna métrica) de su capacidad de generalización. En cambio, tenemos información respecto a la capacidad del modelo respecto al dataset.

### f) Luego de seleccionar la mejor configuración, es ideal volver a correr el algoritmo pero esta vez utilizando todos los datos de desarrollo para luego evaluarlo en los datos de evaluación.

Falso (a medias). Es una estrategia útil. Si bien uno esperaría que utilizar todos los datos sobre los mejores parámetros resulte en un modelo que mejor generalize, esto podría no suceder (podría haber mejores estrategias). Sí se puede considerar una buena estrategia, ya que el modelo final va a contar con mayor información para aprender, lo que es favorable.
