## 4.7. Ordenar las siguientes acciones para evitar problemas (indentar si hay loops y especificar sobre qué datos se haría)

### a) Mandar reporte final a toda la empresa y submitear paper.

### b) Seleccionar la mejor configuración, entrenar con los datos completos.

### c) Partir los datos en desarrollo - evaluación

### d) Correr un clasificador super simple (lo más sencillo que les suela funcionar) y ver si tiene confianza alta para etiquetas incorrectas.

### e) Entrenar un modelo usando una configuración, testear su performance en datos que no fueron utilizados para entrenar. Guardar el resultado.

### f) Partir en 10 folds.

### g) Correr un proceso que lista los atributos según qué tanto se correlacionan con las etiquetas y conservar sólo el top10 para entrenar el modelo.

### h) Definir la métrica a utilizar (ej: accuracy pesada por clase)

### i) Mirar / escuchar / leer instancias del dataset para pensar buenos atributos.

### j) No me dio tan bien como esperaba. Empiezo de nuevo repensando algunas cosas.

### k) Armar una grilla de configuraciones a probar

### l) Después de correr lo anterior me di cuenta que podría haber probado tal modelo / tal hiperparámetro.

### m) Agregar a las configuraciones y repetir.

### n) Evaluar el modelo entrenado con los datos completos.

### ñ) Plotear la distribución de las etiquetas.

Podemos considerar:

- ñ) Plotear la distribución de las etiquetas.

    - c) Partir los datos en desarrollo y evaluación
    
    - h) Definir la métrica a utilizar (ej: accuracy pesada por clase)
    
    - i) Mirar / escuchar / leer instancias del dataset para pensar buenos atributos.

    - d) Correr un clasificador super simple (lo más sencillo que les suela funcionar) y ver si tiene confianza alta para etiquetas incorrectas.

    - g) Correr un proceso que lista los atributos según qué tanto se correlacionan con las etiquetas y conservar sólo el top10 para entrenar el modelo.
    
    - f) Partir en 10 folds.

        - k) Armar una grilla de configuraciones a probar

        - e) Entrenar un modelo usando una configuración, testear su performance en datos que no fueron utilizados para entrenar. Guardar el resultado.

        - l) Después de correr lo anterior me di cuenta que podría haber probado tal modelo / tal hiperparámetro.

        - m) Agregar a las configuraciones y repetir.

    - b) Seleccionar la mejor configuración, entrenar con los datos completos.


    - n) Evaluar el modelo entrenado con los datos completos.

    - j) No me dio tan bien como esperaba. Empiezo de nuevo repensando algunas cosas.

- a) Mandar reporte final a toda la empresa y submitear paper.
