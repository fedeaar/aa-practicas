## 4.5. Preguntas para desarrollar.

### a) En la clase teórica vimos que al hacer cross validation los datos no siempre deben separarse al azar, ¿por qué?. Pensar ejemplos de al menos dos situaciones en las cuales no sea conveniente.

Separar al azar puede resultar en folds donde las instancias no son representativas de la distribución de clases del dataset / problema. En un caso extremo, se podría dar que no haya instancias de clases particulares en algunos folds, llevando a metricas poco representativas.

Otra situación donde podría no convenir es cuando hay alguna correlación 'oculta' entre instancias (forman grupos) e importa evitar tener instancias correlacionadas en train y validación simultáneamente.

### b) ¿Por qué se deberían usar una sola vez los datos held-out?

Considero que está relacionado con la confianza que podemos tener en que la métrica de test realmente mide el poder de generalización de un modelo. En el momento en que consideramos la métrica sobre el held-out como un 'target' a optimizar (lo que sucede implícitamente si usamos la información del desempeño final del modelo como motivación para continuar su entrenamiento) es más probable que estemos sobreajustando el modelo.

### c) ¿Qué sería conveniente hacer si luego de un muy buen resultado en desarrollo, encuentro un pésimo resultado en evaluación?

Lo mejor sería conseguir un nuevo dataset (al menos para test) y reempezar el proceso, aprovechando las intuiciones obtenidas.
