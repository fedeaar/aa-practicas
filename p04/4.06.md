## 4.6. La técnica de `SOBREMUESTREO` (oversampling) consiste en crear copias de instancias al azar (a veces agregando mínimas modificaciones en alguno de sus atributos) y asignarle la etiqueta original. Este proceso se utiliza mucho cuando las clases están muy desbalanceadas y no es suficiente la cantidad de instancias de alguna clase para entrenar un clasificador. En estos casos es normal, por ejemplo, sobremuestrear hasta lograr la misma cantidad de instancias para cada clase. Por ejemplo, dado el problema de clasificar entre GATOS, PERROS y CONEJOS, y dado que la cantidad de instancias es 100, 2000 y 20 respectivamente, generamos un nuevo dataset de 2000, 2000, y 2000 instancias, en donde las instancias agregadas son copias de instancias de la clase correspondiente elegidas al azar y a las que se le agrega un poco de ruido a sus columnas numéricas. Describir los pros y contras de aplicar la técnica en los siguientes pasos del proceso. Justificar en términos de posibilidades de sub o sobreestimar la performance de nuestros modelos y concluir cuál es la opción más segura:

### a) Antes de partir en desarrollo - evaluación

- Contra: alteramos la distribución real de los datos. Peor: en evaluación.
- Contra: es posible que aparezcan copias en ambos sets, lo que vuelve mentirosa la medida de 'generalización' que obtengamos. Se 'vieron' durante el entrenamiento.
- Contra: no tenemos garantías, a priori, que agregar ruido no resulte en instancias 'imposibles' para un problema (¿cómo garantizamos que el ruido no produzca instancias de gatos con cinco patas?)
- Contra: si hay errores en los datos copiados, estamos magnificandolos.
- Pro: (en mi opinión) ninguna. Cualquier manipulación de los datos de evaluación es opuesta al objetivo de esta etapa. 

### b) Antes de partir en Folds en desarrollo.

- Contra: generamos los mismos efectos que en el split desarrollo - evaluación en los splits de K-Folds.
- Contra: en clases con gran cantidad de copias, los modelos resultantes van a ser engañosamente 'buenos' para esas clases.
- Contra: reduce la mayor 'robustez' del modelo resultante. A mayor cantidad de repetidos, mayor será el sesgo del modelo final a la 'importancia' de los atributos repetidos respecto a su clase observada.
- Pro: el set de datos se altera solo una vez. Cada modelo a entrenar durante la validación cruzada provendrá del mismo conjunto.

### c) Luego de seleccionar los folds que se utilizarán para entrenar el modelo dentro de las iteraciones de $K$-fold cross val. Aplicarlo sólo a los datos de entrenamiento.

- Contra: las mismas que las anteriores que no son necesariamente especificas al tipo de split.
- Contra: el conjunto de datos a trabajar va a ser distinto por cada split.
- Contra: si un split queda sin una de las clases minoritarias, podríamos evaluar un modelo sin esa clase, acorde a cómo terminemos de detallar la técnica de sobremuestreo.
- Pro: la manipulación de datos no se 'filtra' a los set de validación.

### d) Luego de seleccionar los folds que se utilizarán para entrenar el modelo dentro de las iteraciones de $K$-fold cross val. Aplicarlo tanto a los datos de entrenamiento como a los de validación.

Este caso considero que es similar al de aplicarlo antes de partir los datos de desarrollo, sin la ventaja de contar con un set único de datos que componga todos los folds.


Conclusión: probablemente se sobreestime la performance del modelo sobre las clases minoritarias, en cualquiera de los casos descriptos. Es posible que el modelo subestime la performance sobre las clases mayoritarias, también, dependiendo del algoritmo de entrenamiento usado (si la predicción es una decisión por mayoría, por ejemplo, funcionaría peor para la clase mayoritaria que si no hubieramos sobremuestrado). La opción (c) parecería ser la más segura.
