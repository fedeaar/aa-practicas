
## 10.5.

### a) Escribir el pseudocódigo de la función “Descenso por gradiente”, comentando brevemente qué se espera de cada argumento (junto a su tipo).

0. definir $z$ de manera aleatoria, tomar $X$, $y$, $g$ (una función de pérdida), $\nabla g$ como entrada y $\alpha$ el _learning rate_.

1. hasta satisfacer algún criterio de parada, como $\nabla g(z) \approx 0$:

    i. dados los parámetros actuales $z$, calcular $\nabla g(z)$.

    ii. actualizar $z := z - \alpha \nabla g(z)$

### b) Comentar cuál es la función a minimizar en el caso de una regresión lineal.

MSE

### c) Explicar cómo se obtiene el gradiente de la función a minimizar en el caso de una regresión lineal.

Se puede derivar analíticamente.

### d) Escribir el pseudocódigo de mini-batch gradient descent.

0. definir $z$ de manera aleatoria, tomar $X$, $y$, $g$ (una función de pérdida), $\nabla g$ como entrada y $\alpha$ el _learning rate_.

1. hasta satisfacer algún criterio de parada, como $\nabla g(z) \approx 0$:

    2. mientras haya datos para samplear un mini batch en esta iteración (el tamaño y método de sampleo podrían tomarse por parametro):

        3. samplear un mini batch de $X$ 

        4. calcular $\nabla g(z)$ sobre el mini batch.

        5. actualizar $z := z - \alpha \nabla g(z)$
