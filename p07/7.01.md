
## 7.1. Queremos demostrar el Bias-Variance decomposition para problemas de regresión: 

$$
\begin{split}
Error\_esperado(x^{(i)}; L) 
    &= \text{E}_{D_n}[error(y^{(i)}, \hat{h}_{L, D_n}(x^{(i)}))] \\
    &\stackrel{reg}{=}(\text{Bias}[\hat{h}_{L, D_n}(x^{(i)}); f(x^{(i)})])^2 + \text{Var}[\hat{h}_{L, D_n}(x^{(i)})] + \text{Var}(\varepsilon)
\end{split}
$$

### a) Según lo visto en clase, ¿cuál es la métrica de $error$ que utilizamos en regresión para la descomposición de sesgo-varianza?

Se suele usar MSE.

### b) Demostrar la descomposición de sesgo y varianza haciendo las suposiciones hechas en clase.

Notar que
$$
\begin{split}
    \text{E}_{D_n}[error(y^{(i)}, \hat{h}_{L, D_n}(x^{(i)}))]
        &= \text{E}_{D_n}[error(f(x^{(i)}) + \varepsilon, \hat{h}_{L, D_n}(x^{(i)}))]\\
        &\stackrel{reg}{=} \text{E}_{D_n}[(f(x^{(i)}) + \varepsilon - \hat{h}_{L, D_n}(x^{(i)}))^2]\\
        &= \text{E}_{D_n}[f(x^{(i)})^2 + \varepsilon^2 + \hat{h}_{L, D_n}(x^{(i)})^2 + \\
        &\ \ \ \ \ \ \ \ \ \ \ \ \ \ 2\varepsilon f(x^{(i)}) - 2f(x^{(i)})\hat{h}_{L, D_n}(x^{(i)}) - 2\varepsilon\hat{h}_{L, D_n}(x^{(i)}) ]
\end{split}
$$
Por linealidad de la esperanza, dado que $f$ no depende de $D_n$ y dado que $\varepsilon$ es independiente de $\hat{h}$
$$
\begin{split}
        &= f(x^{(i)})^2 + \text{E}_{D_n}[\varepsilon^2] + \text{E}_{D_n}[\hat{h}_{L, D_n}(x^{(i)})^2] + \\
        &\ \ \ \ \ \ \ \ \ \ \ \ \ \ 2f(x^{(i)})\text{E}_{D_n}[\varepsilon ] - 2f(x^{(i)})\text{E}_{D_n}[\hat{h}_{L, D_n}(x^{(i)})] - 2\text{E}_{D_n}[\varepsilon]\text{E}_{D_n}[\hat{h}_{L, D_n}(x^{(i)})]\\
\end{split}  
$$
Dado que $\text{E}_{D_n}[\varepsilon] = 0$ por definición de $\varepsilon$, 
$$
\begin{split}
        &= f(x^{(i)})^2 + \text{E}_{D_n}[\varepsilon^2] + \text{E}_{D_n}[\hat{h}_{L, D_n}(x^{(i)})^2]- 2f(x^{(i)})\text{E}_{D_n}[\hat{h}_{L, D_n}(x^{(i)})]
\end{split}  
$$
Dado que $Var[x] = E[x^2] - E[x]^2$,
$$
\begin{split}
        &= (f(x^{(i)}) - \text{E}_{D_n}[\hat{h}_{L, D_n}(x^{(i)}))])^2  + Var[\hat{h}_{L, D_n}(x^{(i)})] + \text{Var}(\varepsilon)
\end{split}
$$
Dado que $Bias[x; y] = E[x] - y$ y $(a - b)^2 = (b - a)^2$
$$
\begin{split}
    &= \text{Bias}[\hat{h}_{L, D_n}(x^{(i)}); f(x^{(i)})]^2 + \text{Var}[\hat{h}_{L, D_n}(x^{(i)})] + \text{Var}(\varepsilon)
\end{split} 
$$
$\blacksquare$

### c) Si ahora definimos $error$ como $$error(y^{(i)}, pred^{(i)}) = y - pred$$ ¿Se obtiene la misma descomposición? (en caso de no poder llegar a una fórmula aclarar dónde encuentran un problema).

En este caso, siguiendo las mismas suposiciones que antes, llegamos a
$$
\begin{split}
    \text{E}_{D_n}[error(y^{(i)}, \hat{h}_{L, D_n}(x^{(i)}))]
        &= \text{E}_{D_n}[error(f(x^{(i)}) + \varepsilon, \hat{h}_{L, D_n}(x^{(i)}))]\\
        &\stackrel{err}{=} \text{E}_{D_n}[f(x^{(i)}) + \varepsilon - \hat{h}_{L, D_n}(x^{(i)})]\\
        &= f(x^{(i)}) - \text{E}_{D_n}[\hat{h}_{L, D_n}(x^{(i)})]\\
        &= -\text{Bias}[\hat{h}_{L, D_n}(x^{(i)}); f(x^{(i)})]
\end{split}
$$
Luego, no se obtiene la misma descomposición.

$\blacksquare$
